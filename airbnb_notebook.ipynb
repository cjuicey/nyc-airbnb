{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC AirBnB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AB_NYC_2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "name\n",
      "host_id\n",
      "host_name\n",
      "neighbourhood_group\n",
      "neighbourhood\n",
      "latitude\n",
      "longitude\n",
      "room_type\n",
      "price\n",
      "minimum_nights\n",
      "number_of_reviews\n",
      "last_review\n",
      "reviews_per_month\n",
      "calculated_host_listings_count\n",
      "availability_365\n"
     ]
    }
   ],
   "source": [
    "for c in df: print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                  int64\n",
       "name                               object\n",
       "host_id                             int64\n",
       "host_name                          object\n",
       "neighbourhood_group                object\n",
       "neighbourhood                      object\n",
       "latitude                          float64\n",
       "longitude                         float64\n",
       "room_type                          object\n",
       "price                               int64\n",
       "minimum_nights                      int64\n",
       "number_of_reviews                   int64\n",
       "last_review                        object\n",
       "reviews_per_month                 float64\n",
       "calculated_host_listings_count      int64\n",
       "availability_365                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 48895 unique ID's and 48895 entries. Data is sane\n"
     ]
    }
   ],
   "source": [
    "df_id = df.loc[:,'id']\n",
    "def id_sanity():\n",
    "    A = len(df_id.unique())\n",
    "    B = len(df_id)\n",
    "    C = ('is' if A==B else 'is not')\n",
    "    print(\"There are {} unique ID's and {} entries. Data {} sane\".format(A,B,C))\n",
    "    \n",
    "id_sanity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beelow we count the number of hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hosts: 37457 \n",
      "Total bookings: 48895\n"
     ]
    }
   ],
   "source": [
    "def count_hosts():\n",
    "    A = len(df.loc[:,'host_id'].unique())\n",
    "    B = len(df)\n",
    "    print('Unique hosts: {} \\nTotal bookings: {}'.format(A,B))\n",
    "count_hosts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the dataframe by removing columns and replacing NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nyairbnb(df):\n",
    "    df_clean = df\n",
    "    #for now ignore neighbourhoods\n",
    "    df_clean = df_clean.drop('neighbourhood',axis=1)\n",
    "   #name is irrelevant. id and host_id number should not be weighted\n",
    "    df_clean = df_clean.drop(['name','id','host_id'],axis=1)\n",
    "    #zero prices are illogical\n",
    "    df_clean = df_clean[df.price>0]\n",
    "    #host name is unimportant\n",
    "    df_clean = df_clean.drop(['host_name'],axis=1)\n",
    "    #last review date is dropped\n",
    "    df_clean = df_clean.drop(['last_review'],axis=1)\n",
    "    #replace @reviews_per_month NaN values with 25% of mean\n",
    "    col = df_clean.reviews_per_month\n",
    "    filler = round(col.mean()*0.25,2)\n",
    "    col.fillna(filler,inplace=True)\n",
    "    df_clean = df_clean.drop('reviews_per_month',axis=1)\n",
    "    df_clean = pd.concat([df_clean,col],axis=1)\n",
    "    return df_clean\n",
    "\n",
    "df_cleaned = clean_nyairbnb(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categorical columns using one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cleaned = pd.get_dummies(df_cleaned,dummy_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#shuffle the data first\n",
    "df_cleaned = df_cleaned.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = list(filter(lambda a: a != 'price', df_cleaned.columns))\n",
    "x = df_cleaned[x_cols]\n",
    "train_num = int(0.75*len(df_cleaned)) \n",
    "x_train = x.iloc[0:train_num]\n",
    "x_test = x.iloc[train_num:]\n",
    "y = df_cleaned['price'].astype('float32')\n",
    "y_train = y.iloc[0:train_num]\n",
    "y_test = y.iloc[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check multiple correlation  \n",
    "latitude + longitude ~ price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = df.loc[:,'latitude']\n",
    "longitude = df.loc[:,'longitude']\n",
    "price = df.loc[:,'price']\n",
    "df_dict = {\"latitude\": latitude, \"longitude\" : longitude, \"price\" : price}\n",
    "location_price_data = pandas.DataFrame(df_dict)     \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ols(\"price ~ longitude + latitude\", data=location_price_data).fit()\n",
    "print(model.rsquared**.5)\n",
    "#print(model.params)\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See result for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(x_train,y_train)\n",
    "y_pred = linreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229.219706644271\n"
     ]
    }
   ],
   "source": [
    "RMSE = (sum((y_pred - np.array(y_test))**2)/len(x_test))**(0.5)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see the root mean squared error (RMSE) is very high. Indeed as a percentage of the average value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 149.44% of average price\n"
     ]
    }
   ],
   "source": [
    "ratio = (RMSE/np.mean(y_test))*100\n",
    "print(\"RMSE is {0:.2f}% of average price\".format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = RandomForestRegressor(n_estimators=50)\n",
    "rfreg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.079942916387713\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfreg.predict(x_test)\n",
    "RMSE_forest = (sum((y_test - y_pred)**2))**(0.5) \n",
    "print(RMSE_forest/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty good error margin one would think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put things in np arrays, which tf understands\n",
    "#X_data = np.array(x)\n",
    "#y_data = np.array(y)\n",
    "#X_data = tf.placeholder(dtype = tf.float32,shape=[1,16])\n",
    "#y_data = tf.placeholder(dtype = tf.float32,shape=[1])\n",
    "x = np.array(x).astype('float32')\n",
    "y = np.array(y).astype('float32')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "X_data, y_data = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network will have 1 input layer, 1 hidden layer and 1 output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x.shape[1]\n",
    "num_nodes_1 = 10 \n",
    "num_nodes_2 = 1\n",
    "X_data = tf.reshape(X_data,[1,input_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first hidden layer is defined in terms of the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = tf.Variable(tf.random_uniform([input_dim,num_nodes_1]),dtype=tf.float32)\n",
    "b_1 = tf.Variable(tf.zeros([num_nodes_1]),dtype=tf.float32)\n",
    "layer_1 = tf.add(tf.matmul(X_data,W_1), b_1)\n",
    "layer_1 = tf.nn.relu(layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_2 = tf.Variable(tf.random_uniform([num_nodes_1,num_nodes_2]),dtype=tf.float32)\n",
    "b_2 = tf.Variable(tf.zeros([num_nodes_2]),dtype=tf.float32)\n",
    "layer_2 = tf.add(tf.matmul(layer_1,W_2), b_2)\n",
    "layer_2 = tf.nn.relu(layer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the output layer is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_out = tf.Variable(tf.random_uniform([num_nodes_2,1]))\n",
    "b_out = tf.Variable(tf.zeros([1]))\n",
    "output = tf.add(tf.matmul(layer_2,W_out), b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(output - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.746934388041077\n",
      "Epoch 1: 5.388221510361263\n",
      "Epoch 2: 5.4884628921432626\n",
      "Epoch 3: 5.577537831908191\n",
      "Epoch 4: 5.923117178550242\n",
      "Epoch 5: 6.740957227825056\n",
      "Epoch 6: 6.865122473051637\n",
      "Epoch 7: 7.159170627867119\n",
      "Epoch 8: 8.385761575314136\n",
      "Epoch 9: 8.669729425224128\n",
      "Epoch 10: 8.89438844341643\n",
      "Epoch 11: 10.782109566381\n",
      "Epoch 12: 10.985508792578477\n",
      "Epoch 13: 11.099641568160722\n",
      "Epoch 14: 11.621234698709573\n",
      "Epoch 15: 11.89011767054724\n",
      "Epoch 16: 12.093150915544324\n",
      "Epoch 17: 13.580073560223566\n",
      "Epoch 18: 14.394666045750526\n",
      "Epoch 19: 15.053862753773602\n",
      "Epoch 20: 15.101932226914098\n",
      "Epoch 21: 15.167154656583378\n",
      "Epoch 22: 16.641411475020902\n",
      "Epoch 23: 17.161182171018062\n",
      "Epoch 24: 17.887036841002104\n",
      "Epoch 25: 18.34321239019765\n",
      "Epoch 26: 18.36455577153951\n",
      "Epoch 27: 18.72801649616311\n",
      "Epoch 28: 18.826167819640627\n",
      "Epoch 29: 18.96122641938377\n",
      "Epoch 30: 19.059292440474515\n",
      "Epoch 31: 19.260695211255857\n",
      "Epoch 32: 19.38902504086524\n",
      "Epoch 33: 19.70452095258354\n",
      "Epoch 34: 20.159673951903358\n",
      "Epoch 35: 21.753559476819486\n",
      "Epoch 36: 21.865815040532816\n",
      "Epoch 37: 22.52172502961411\n",
      "Epoch 38: 22.8115425056799\n",
      "Epoch 39: 23.15197769887962\n",
      "Epoch 40: 23.263929699965736\n",
      "Epoch 41: 23.298254593803218\n",
      "Epoch 42: 23.752133770783416\n",
      "Epoch 43: 23.948296917371515\n",
      "Epoch 44: 24.03183873380014\n",
      "Epoch 45: 24.371688636759185\n",
      "Epoch 46: 26.138269343125273\n",
      "Epoch 47: 27.964757815836183\n",
      "Epoch 48: 28.023853741186883\n",
      "Epoch 49: 28.223285867016656\n",
      "Epoch 50: 28.2718374035224\n",
      "Epoch 51: 28.471150425644925\n",
      "Epoch 52: 28.494366224342162\n",
      "Epoch 53: 28.73599162756075\n",
      "Epoch 54: 29.02402210852078\n",
      "Epoch 55: 29.07241095120032\n",
      "Epoch 56: 29.23308885426229\n",
      "Epoch 57: 29.31585664142808\n",
      "Epoch 58: 29.703152131452935\n",
      "Epoch 59: 29.751406363170496\n",
      "Epoch 60: 30.55791625776791\n",
      "Epoch 61: 30.627968525556835\n",
      "Epoch 62: 30.676101371178934\n",
      "Epoch 63: 30.8745653249074\n",
      "Epoch 64: 30.903298003029175\n",
      "Epoch 65: 31.348176160056525\n",
      "Epoch 66: 31.5424046604989\n",
      "Epoch 67: 31.590367220981687\n",
      "Epoch 68: 32.04094297813126\n",
      "Epoch 69: 32.37287372314113\n",
      "Epoch 70: 32.558881690503924\n",
      "Epoch 71: 32.7526981824159\n",
      "Epoch 72: 32.79287613847722\n",
      "Epoch 73: 32.948813346411434\n",
      "Epoch 74: 34.855425492001\n",
      "Epoch 75: 35.50525060412868\n",
      "Epoch 76: 37.26376294328873\n",
      "Epoch 77: 37.37318280195606\n",
      "Epoch 78: 37.68833960172695\n",
      "Epoch 79: 37.71802927474033\n",
      "Epoch 80: 37.91485124742055\n",
      "Epoch 81: 38.56350302222161\n",
      "Epoch 82: 39.21189537388575\n",
      "Epoch 83: 40.73648387677014\n",
      "Epoch 84: 41.49802554940331\n",
      "Epoch 85: 41.69410970638079\n",
      "Epoch 86: 41.79401895822572\n",
      "Epoch 87: 41.820341604195484\n",
      "Epoch 88: 41.9746945849925\n",
      "Epoch 89: 42.01224984475765\n",
      "Epoch 90: 42.89590801703795\n",
      "Epoch 91: 43.004314844936644\n",
      "Epoch 92: 43.61501908386195\n",
      "Epoch 93: 43.75849460195709\n",
      "Epoch 94: 43.934400988760544\n",
      "Epoch 95: 44.074351131013756\n",
      "Epoch 96: 44.873868467335456\n",
      "Epoch 97: 44.95417695872259\n",
      "Epoch 98: 45.015258936393906\n",
      "Epoch 99: 45.12317800244824\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 30\n",
    "n_samples = x.shape[0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tot_loss = 0\n",
    "    for i in range(100):\n",
    "        _, l = sess.run([optimizer,cost])\n",
    "        tot_loss += l\n",
    "        print('Epoch {0}: {1}'.format(i, tot_loss/n_samples))\n",
    "    #print('Loss: {}'.format(tot_loss/n_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an out-of-the-box neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = (10,)\n",
    "mlpreg = MLPRegressor(hidden_layer_sizes = num_neurons, activation = 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjuicey/.local/lib/python3.4/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpreg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = mlpreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25186.91837434137\n"
     ]
    }
   ],
   "source": [
    "RMSE_nn = (sum((y_pred_nn - y_test)**2))**(0.5)\n",
    "RMSE_nn/len(y_test)\n",
    "print(RMSE_nn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
